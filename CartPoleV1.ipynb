{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param\n",
    "l = 0.98 # lambda\n",
    "a = 0.1 # learning rate\n",
    "eg = 0.9 # epsilon greedy\n",
    "egMin = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Agent, self).__init__(name='mon_agent')\n",
    "    # Define your layers here.\n",
    "    self.dense1 = tf.keras.layers.Dense(8, activation='relu', input_shape=(4,))\n",
    "    self.dense2 = tf.keras.layers.Dense(2, activation='relu')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    tmp = self.dense1(inputs)\n",
    "    return self.dense2(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coach(tf.keras.Model):\n",
    "    def __init__(self, agent):\n",
    "        super(Coach, self).__init__()\n",
    "        self.agent = agent\n",
    "\n",
    "    def call(self, inputs):\n",
    "        [states, masks] = inputs\n",
    "        return agent(states) * masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showHistory(history) :\n",
    "    #print(history.history.keys())\n",
    "    plt.figure(figsize=(15.0,10.0))\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2) \n",
    "    fig.set_size_inches(15.0, 7.0)         \n",
    "    axes[0].plot(history.history['loss'], label=\"loss\")\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(history.history['accuracy'], label=\"accuracy\")\n",
    "    axes[1].legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "agent.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "agent.build(tf.TensorShape([None,4]))\n",
    "\n",
    "coach = Coach(agent)\n",
    "coach.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=\"mse\",\n",
    "    metrics=['accuracy'])\n",
    "coach([tf.keras.Input(shape=(4)), tf.keras.Input(shape=(2))])\n",
    "\n",
    "\n",
    "agent.summary()\n",
    "coach.summary()\n",
    "\n",
    "#print(agent([[0,0,0,0]]));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 0\n",
    "#r = 0\n",
    "s0 = None\n",
    "o0 = None\n",
    "for epi in range(1,100) :\n",
    "    memory = { \"states\":[], \"masks\":[], \"actions\":[], \"values\":[]}\n",
    "    state = env.reset()\n",
    "    c0 = 0\n",
    "    c1 = 0\n",
    "    # * * * explore * * *\n",
    "    for step in range(500): #epi * 100):\n",
    "        env.render()\n",
    "        #print(state)\n",
    "        if (random.random() > eg) :\n",
    "            c0 +=1\n",
    "            Qs = agent([state])[0];\n",
    "            action = np.argmax(Qs);\n",
    "            Q = Qs[action]\n",
    "            #print(\"State: \", state, \" Qs: \", Qs.numpy(), \" Action: \", action, \" Q: \", Q.numpy())\n",
    "        else :\n",
    "            c1 +=1\n",
    "            Qs = agent([state])[0];\n",
    "            action = random.randrange(2)\n",
    "            Q = Qs[action]\n",
    "            #print(\"State: \", state, \" Action: \", action, \" Q: \", Q.numpy())\n",
    "\n",
    "        memory[\"states\"].append(state)\n",
    "        memory[\"actions\"].append(action)\n",
    "        mask = np.zeros(2)\n",
    "        mask[action] = 1\n",
    "        memory[\"masks\"].append(mask)\n",
    "        #if o0 == None : s0 = state\n",
    "        state, rewards, done, info = env.step(action)\n",
    "        #input()\n",
    "\n",
    "        Q0 = Q    \n",
    "        if not done : \n",
    "            #rewards -= 1.0\n",
    "            Qs = agent([state])[0];\n",
    "            Q = Q + a * (rewards + l * np.max(Qs) - Q)\n",
    "            Q = Q.numpy()\n",
    "        else :\n",
    "            #rewards = -1.0\n",
    "            rewards = 0.0\n",
    "            #print(Q.numpy(), ' => ')\n",
    "            Q = Q + a * (rewards - Q)\n",
    "            Q = 0\n",
    "            print('.', end='')\n",
    "            #if o0 == None : o0 = Q #.numpy()\n",
    "\n",
    "        \n",
    "        #print(rewards, done, \" Q : \", Q0.numpy(), \" => \", Q.numpy())\n",
    "        #print(\"rewards: \", rewards, \" Qs: \", Qs.numpy(), \" Q: \", Q.numpy(), \" mask: \", mask)\n",
    "        \n",
    "        memory[\"values\"].append(mask * Q) #.numpy())\n",
    "        #memory[\"targets\"].append( { \"mask\": mask, \"action\":action, \"value\":Q.numpy() } )\n",
    "        if done : state = env.reset()\n",
    "        #if rewards == 0 : state = env.reset()\n",
    "        \n",
    "    #print(memory)\n",
    "    eg = max(egMin, eg*.9)\n",
    "    print(\"\\r\\neG: \", eg, \"; \", c0,\"-\",c1,\"/\",c0+c1)\n",
    "    \n",
    "    # * * * learn * * *\n",
    "    history = coach.fit(\n",
    "        [np.array(memory[\"states\"]), np.array(memory[\"masks\"])],\n",
    "        np.array(memory[\"values\"]),\n",
    "        epochs=10, batch_size=64, verbose=0)\n",
    "    \n",
    "    #showHistory(history)\n",
    "    \n",
    "    #print(agent([s0])[0].numpy(), \" <= \", o0)\n",
    "    #break\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()\n",
    "agent.save_weights('CartPoleV1.tf')\n",
    "#agent.load_weights('CartPoleV1.tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eg = 0.05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
