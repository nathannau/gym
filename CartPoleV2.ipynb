{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param\n",
    "l = 0.98 # lambda\n",
    "a = 0.15 # learning rate\n",
    "eg = 0.10 # epsilon greedy\n",
    "egMin = 0.05\n",
    "maxBufferSize = 50000 #500000\n",
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Agent, self).__init__(name='mon_agent')\n",
    "    # Define your layers here.\n",
    "    #self.dense1 = tf.keras.layers.Dense(16, activation='relu', input_shape=(4,))\n",
    "    self.dense1 = tf.keras.layers.Dense(24, activation='relu')\n",
    "    self.dropout1 = tf.keras.layers.Dropout(0.5)\n",
    "    self.dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "    self.dropout2 = tf.keras.layers.Dropout(0.5)\n",
    "    self.dense3 = tf.keras.layers.Dense(2, activation='linear')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    tmp = inputs\n",
    "    tmp = self.dense1(tmp)\n",
    "    tmp = self.dropout1(tmp)\n",
    "    tmp = self.dense2(tmp)\n",
    "    tmp = self.dropout2(tmp)\n",
    "    return self.dense3(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coach(tf.keras.Model):\n",
    "    def __init__(self, agent):\n",
    "        super(Coach, self).__init__()\n",
    "        self.agent = agent\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        [states, masks] = inputs\n",
    "        #tf.print(\"states : \", states)\n",
    "        #tf.print(\"masks : \", masks)\n",
    "        return agent(states, **kwargs) * masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showHistory(history) :\n",
    "    #print(history.history.keys())\n",
    "    plt.figure(figsize=(15.0,10.0))\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2) \n",
    "    fig.set_size_inches(15.0, 7.0)         \n",
    "    axes[0].plot(history.history['loss'], label=\"loss\")\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(history.history['accuracy'], label=\"accuracy\")\n",
    "    axes[1].legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "agent.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "agent.build(tf.TensorShape([None,4]))\n",
    "#agent.build(tf.TensorShape([None,2]))\n",
    "\n",
    "coach = Coach(agent)\n",
    "#coach.compile(optimizer=tf.keras.optimizers.Adam(1e-6),\n",
    "coach.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=\"mse\",\n",
    "    metrics=['accuracy'])\n",
    "coach([tf.keras.Input(shape=(4,)), tf.keras.Input(shape=(2,))])\n",
    "#coach([tf.keras.Input(shape=(2,)), tf.keras.Input(shape=(2,))])\n",
    "\n",
    "\n",
    "agent.summary()\n",
    "coach.summary()\n",
    "\n",
    "#print(agent([[0,0,0,0]]));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()\n",
    "#agent.save_weights('CartPoleV1.tf')\n",
    "#agent.load_weights('CartPoleV1.tf')\n",
    "#history = coach.fit(\n",
    "#    [np.array(memory[\"states\"]), np.array(memory[\"masks\"])],\n",
    "#    np.array(memory[\"values\"]),\n",
    "#    epochs=10, batch_size=64, verbose=1)\n",
    "#by = [ agent([b['sp']])          for b in batch ]\n",
    "#print(by)\n",
    "#np.array(by * bm),\n",
    "#by = [ b[\"m\"] * ( b['r'] if b['d'] else b['r'] + a * np.max(agent([b['sp']])[0]) )\n",
    "#           for b in batch ]\n",
    "#print(by, bm)\n",
    "#print(pix)\n",
    "def drawAgent(agent, p=0.0, dp=0.0) :\n",
    "    rx = np.arange(-0.24,0.24,0.02, np.float32)\n",
    "    ry = np.arange(-2,2,0.2, np.float32)\n",
    "    def agentDir(x,y) :\n",
    "        [a ,b] = agent([[p, dp, x, y]])[0]\n",
    "        #return '<' if b<a else '>'\n",
    "        #return (b-a).numpy()\n",
    "        return np.sign(b-a)\n",
    "        \n",
    "    pix = [\n",
    "        #[ [[type(0.0), type(0.0), type(x), type(y)]] for x in rx]\n",
    "        [ agentDir(x, y) for x in rx]\n",
    "        for y in ry\n",
    "        ]\n",
    "    #print(pix)\n",
    "    #return\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(15.0, 7.0)         \n",
    "    im = ax.imshow(pix)\n",
    "    ax.set_xticks(np.arange(len(rx)))\n",
    "    ax.set_yticks(np.arange(len(ry)))\n",
    "    ax.set_xticklabels([round(l,2) for l in rx])\n",
    "    ax.set_yticklabels([round(l,2) for l in ry])\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")    \n",
    "    for y in range(len(ry)):\n",
    "        for x in range(len(rx)):\n",
    "            #text = ax.text(x, y, pix[y][x], ha=\"center\", va=\"center\", color=\"w\")\n",
    "            text = ax.text(x, y, '<' if pix[y][x]<0 else '>' if pix[y][x]>0 else '0', ha=\"center\", va=\"center\", color=\"w\")\n",
    "    plt.show()\n",
    "\n",
    "#print(pix)\n",
    "drawAgent(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = [] #{ \"states\":[], \"masks\":[], \"actions\":[], \"values\":[]}\n",
    "loss = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epi in range(1,100) :\n",
    "    #memory = { \"states\":[], \"masks\":[], \"actions\":[], \"values\":[]}\n",
    "    state = env.reset() #[2:]\n",
    "    #state *= [0.0, 0.0, 1.0, 1.0]\n",
    "    c0 = 0\n",
    "    c1 = 0\n",
    "    # * * * explore * * *\n",
    "    print(epi, \" => \", eg)\n",
    "    nbTry = 0\n",
    "    slm = 0\n",
    "    slc = 0\n",
    "    for step in range(1000): #epi * 100):\n",
    "        env.render()\n",
    "        #print(state)\n",
    "        if (random.random() > eg) :\n",
    "            c0 +=1\n",
    "            Qs = agent([state])[0];\n",
    "            action = np.argmax(Qs);\n",
    "            Q = Qs[action]\n",
    "            #print(\"State: \", state, \" Qs: \", Qs.numpy(), \" Action: \", action, \" Q: \", Q.numpy())\n",
    "        else :\n",
    "            c1 +=1\n",
    "            Qs = agent([state])[0];\n",
    "            action = random.randrange(2)\n",
    "            Q = Qs[action]\n",
    "            #print(\"State: \", state, \" Action: \", action, \" Q: \", Q.numpy())\n",
    "\n",
    "        mask = np.zeros(2)\n",
    "        mask[action] = 1\n",
    "        mem = { \"s\": state, \"a\": action, \"m\": mask}\n",
    "        #sm = env.state\n",
    "        state, rewards, done, info = env.step(action)\n",
    "        #state *= [0.0, 0.0, 1.0, 1.0]\n",
    "        #state = state[2:]\n",
    "        \n",
    "        #### CHEAT\n",
    "        #if env.state[0] > 1.5 or env.state[0] < -1.5 : env.state = (0.0, env.state[1], env.state[2], env.state[3]); print('|',end='')\n",
    "        #env.state = (0.0, 0.0, env.state[2], env.state[3])\n",
    "\n",
    "        if done : rewards = 0.0\n",
    "        rewards -= 1.0\n",
    "        mem[\"r\"] = rewards\n",
    "        mem[\"sp\"] = state\n",
    "        mem[\"d\"] = done\n",
    "        memory.append(mem)\n",
    "        #input()\n",
    "        #print(mem)\n",
    "        #Q0 = Q    \n",
    "        #if not done : \n",
    "        #    #rewards -= 1.0\n",
    "        #    Qs = agent([state])[0];\n",
    "        #    #Q = Q + a * (rewards + l * np.max(Qs) - Q)\n",
    "        #    Q = rewards + l * np.max(Qs)\n",
    "        #    #Q = Q.numpy()\n",
    "        #else :\n",
    "        #    #rewards = -1.0\n",
    "        #    rewards = 0.0\n",
    "        #    #print(Q.numpy(), ' => ')\n",
    "        #    Q = Q + a * (rewards - Q)\n",
    "        #    #Q = 0.0\n",
    "        #    print('.', end='')\n",
    "        #    #if o0 == None : o0 = Q #.numpy()\n",
    "\n",
    "        \n",
    "        #print(rewards, done, \" Q : \", Q0.numpy(), \" => \", Q.numpy())\n",
    "        #print(\"rewards: \", rewards, \" Qs: \", Qs.numpy(), \" Q: \", Q.numpy(), \" mask: \", mask)\n",
    "        \n",
    "        #memory[\"values\"].append(mask * Q) #.numpy())\n",
    "        #memory[\"targets\"].append( { \"mask\": mask, \"action\":action, \"value\":Q.numpy() } )\n",
    "        slc += 1\n",
    "        if done : \n",
    "            #print('.', end=''); \n",
    "            print('-' if abs(state[0])>2.4 else 'a' if abs(state[2])>0.2094 else 'len?', end=''); \n",
    "            if slc>slm : slm = slc\n",
    "            slc = 0\n",
    "            nbTry += 1\n",
    "            #print(nbTry, \" \", Qs.numpy(), \" \", action, \" \", sm, \"=>\", state, 'pos' if abs(state[0])>2.4 else 'angle' if abs(state[2])>0.2094 else 'len?');\n",
    "            state = env.reset() #[2:]\n",
    "            #state *= [0.0, 0.0, 1.0, 1.0]\n",
    "        #if rewards == 0 : state = env.reset()\n",
    "        \n",
    "            if len(memory)>500 :\n",
    "                while len(memory) > maxBufferSize :\n",
    "                    memory = random.sample(memory, maxBufferSize)\n",
    "                    #index = random.randrange(maxBufferSize)\n",
    "                    #del memory[\"states\"][index]\n",
    "                    #del memory[\"masks\"][index]\n",
    "                    #del memory[\"actions\"][index]\n",
    "                    #del memory[\"values\"][index]\n",
    "\n",
    "                # * * * learn * * *\n",
    "\n",
    "                batch = random.sample(memory, batchSize)\n",
    "                bs = [ b[\"s\"] for b in batch ]\n",
    "                bm = [ b[\"m\"] for b in batch ]\n",
    "                by = [ b[\"m\"] * ( b['r'] if b['d'] else b['r'] + a * np.max(agent([b['sp']])[0]) )\n",
    "                       for b in batch ]\n",
    "\n",
    "                #print(\"bs: \", bs[:4], \" bm: \",bm[:4], \"by: \", by[:4], \"\\r\\n\")\n",
    "                history = coach.fit(\n",
    "                    [np.array(bs), np.array(bm)],\n",
    "                    np.array(by),\n",
    "                    verbose=0)\n",
    "                loss.append(history.history['loss'])\n",
    "                accuracy.append(history.history['accuracy'])\n",
    "            #print(history.history['loss'])\n",
    "        #if step%200==0 : drawAgent(agent)\n",
    "    drawAgent(agent)\n",
    "    print(\"len max : \", slm, \" len moy\", 1000/nbTry)\n",
    "    #print(memory)\n",
    "    eg = max(egMin, eg*.95)\n",
    "    #print(\"\\r\\neG: \", eg, \"; \", c0,\"-\",c1,\"/\",c0+c1)\n",
    "    #print(\"\\r\\neG: \", eg)\n",
    "    \n",
    "    #plt.figure(figsize=(15.0,7.0))\n",
    "    #fig, axes = plt.subplots(nrows=1, ncols=2) \n",
    "    #fig.set_size_inches(15.0, 7.0)         \n",
    "    #axes[0].plot(loss, label=\"loss\")\n",
    "    #axes[0].legend()\n",
    "    #axes[1].plot(accuracy, label=\"accuracy\")\n",
    "    #axes[1].legend()\n",
    "    #plt.show()\n",
    "    #loss = []\n",
    "    #accuracy = []\n",
    "\n",
    "    #showHistory(history)\n",
    "    \n",
    "    #print(agent([s0])[0].numpy(), \" <= \", o0)\n",
    "    #break\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eg = 0.05\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = '<' if a<0 else '>' if a>0 else '0'\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = env.reset() #[2:]\n",
    "while True :\n",
    "    env.render()\n",
    "\n",
    "    Qs = agent([state])[0];\n",
    "    action = np.argmax(Qs);\n",
    "    Q = Qs[action]\n",
    "    print(state, \" \", Qs.numpy(), \" \", action)\n",
    "    drawAgent(agent, state[0], state[1])\n",
    "    \n",
    "    state, rewards, done, info = env.step(action)\n",
    "    input()\n",
    "    if done :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "memory = [] #{ \"states\":[], \"masks\":[], \"actions\":[], \"values\":[]}\n",
    "loss = []\n",
    "accuracy = []\n",
    "\n",
    "state = env.reset()\n",
    "for step in range(1000):\n",
    "    env.render()\n",
    "    #print(state)\n",
    "    if (random.random() > eg) :\n",
    "        c0 +=1\n",
    "        Qs = agent([state])[0];\n",
    "        action = np.argmax(Qs);\n",
    "        Q = Qs[action]\n",
    "        #print(\"State: \", state, \" Qs: \", Qs.numpy(), \" Action: \", action, \" Q: \", Q.numpy())\n",
    "    else :\n",
    "        c1 +=1\n",
    "        Qs = agent([state])[0];\n",
    "        action = random.randrange(2)\n",
    "        Q = Qs[action]\n",
    "        #print(\"State: \", state, \" Action: \", action, \" Q: \", Q.numpy())\n",
    "\n",
    "    mask = np.zeros(2)\n",
    "    mask[action] = 1\n",
    "    mem = { \"s\": state, \"a\": action, \"m\": mask}\n",
    "    print(state, \" \", Qs.numpy(), \" \", action)\n",
    "    drawAgent(agent)\n",
    "    state, rewards, done, info = env.step(action)\n",
    "\n",
    "    if done : rewards = 0.0\n",
    "    mem[\"r\"] = rewards\n",
    "    mem[\"sp\"] = state\n",
    "    mem[\"d\"] = done\n",
    "    memory.append(mem)\n",
    "    \n",
    "    if done : \n",
    "        print(memory)\n",
    "        break\n",
    "\n",
    "\n",
    "batch = memory #random.sample(memory, batchSize)\n",
    "bs = [ b[\"s\"] for b in batch ]\n",
    "bm = [ b[\"m\"] for b in batch ]\n",
    "by = [ b[\"m\"] * ( b['r'] if b['d'] else b['r'] + a * np.max(agent([b['sp']])[0]) )\n",
    "       for b in batch ]\n",
    "\n",
    "print (bs, bm, by)\n",
    "#history = coach.fit(\n",
    "#    [np.array(bs), np.array(bm)],\n",
    "#    np.array(by),\n",
    "#    verbose=0)\n",
    "#loss.append(history.history['loss'])\n",
    "#accuracy.append(history.history['accuracy'])\n",
    "\n",
    "drawAgent(agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
