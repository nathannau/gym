{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "\n",
    "#EXPLORATION_MAX = 1.0 # epsilon greedy\n",
    "#EXPLORATION_MIN = 0.01\n",
    "#EXPLORATION_DECAY = 0.995\n",
    "\n",
    "MEMORY_SIZE = 100 #1000000 #50000\n",
    "#BATCH_SIZE = 100 #20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Agent, self).__init__()\n",
    "    # Define your layers here.\n",
    "    self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "    #self.dropout1 = tf.keras.layers.Dropout(0.5)\n",
    "    #self.dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "    #self.dropout2 = tf.keras.layers.Dropout(0.5)\n",
    "    self.dense2 = tf.keras.layers.Dense(2, activation='softmax')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    tmp = tf.convert_to_tensor(inputs)\n",
    "    tmp = self.dense1(tmp)\n",
    "    tmp = self.dense2(tmp)\n",
    "    \n",
    "    logits = tmp\n",
    "    logs_pi = tf.math.log(logits)\n",
    "    return logits, logs_pi\n",
    "\n",
    "  def explore(self, inputs):\n",
    "    logits, logs_pi = self(inputs)\n",
    "    return tf.squeeze(tf.random.categorical(logits, 1)), tf.squeeze(logs_pi)\n",
    "\n",
    "  def exploit(self, inputs):\n",
    "    logits, _ = self(inputs)\n",
    "    return tf.math.argmax(logits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"agent_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              multiple                  160       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  66        \n",
      "=================================================================\n",
      "Total params: 226\n",
      "Trainable params: 226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "#agent.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "agent.build(tf.TensorShape([None,4]))\n",
    "agent.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "    loss=\"mse\",\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "agent.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def showHistory(history) :\n",
    "#    #print(history.history.keys())\n",
    "#    plt.figure(figsize=(15.0,10.0))\n",
    "#    fig, axes = plt.subplots(nrows=1, ncols=2) \n",
    "#    fig.set_size_inches(15.0, 7.0)         \n",
    "#    axes[0].plot(history.history['loss'], label=\"loss\")\n",
    "#    axes[0].legend()\n",
    "#    axes[1].plot(history.history['accuracy'], label=\"accuracy\")\n",
    "#    axes[1].legend()\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##env.close()\n",
    "##agent.save_weights('CartPoleV1.tf')\n",
    "##agent.load_weights('CartPoleV1.tf')\n",
    "#\n",
    "#def drawAgent(agent, p=0.0, dp=0.0) :\n",
    "#    rx = np.arange(-0.24,0.24,0.02, np.float32)\n",
    "#    ry = np.arange(-2,2,0.2, np.float32)\n",
    "#    def agentDir(x,y) :\n",
    "#        [a ,b] = agent([[p, dp, x, y]])[0]\n",
    "#        #return '<' if b<a else '>'\n",
    "#        #return (b-a).numpy()\n",
    "#        return np.sign(b-a)\n",
    "#        \n",
    "#    pix = [\n",
    "#        #[ [[type(0.0), type(0.0), type(x), type(y)]] for x in rx]\n",
    "#        [ agentDir(x, y) for x in rx]\n",
    "#        for y in ry\n",
    "#        ]\n",
    "#    #print(pix)\n",
    "#    #return\n",
    "#    fig, ax = plt.subplots()\n",
    "#    fig.set_size_inches(15.0, 7.0)         \n",
    "#    im = ax.imshow(pix)\n",
    "#    ax.set_xticks(np.arange(len(rx)))\n",
    "#    ax.set_yticks(np.arange(len(ry)))\n",
    "#    ax.set_xticklabels([round(l,2) for l in rx])\n",
    "#    ax.set_yticklabels([round(l,2) for l in ry])\n",
    "#    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")    \n",
    "#    for y in range(len(ry)):\n",
    "#        for x in range(len(rx)):\n",
    "#            #text = ax.text(x, y, pix[y][x], ha=\"center\", va=\"center\", color=\"w\")\n",
    "#            text = ax.text(x, y, '<' if pix[y][x]<0 else '>' if pix[y][x]>0 else '0', ha=\"center\", va=\"center\", color=\"w\")\n",
    "#    plt.show()\n",
    "#\n",
    "##print(pix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = [] #{ \"states\":[], \"masks\":[], \"actions\":[], \"values\":[]}\n",
    "#loss = []\n",
    "#accuracy = []\n",
    "#eg = EXPLORATION_MAX\n",
    "#scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1,5000) :\n",
    "    print(\"Epoch : \", epoch)\n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    epoch_memory = [] #np.array()\n",
    "    #for step in range(1000):\n",
    "    while True :\n",
    "        step += 1\n",
    "        #if eg<0.03 :\n",
    "        env.render()\n",
    "        #    #env.render()\n",
    "        #    pass\n",
    "        #input()\n",
    "        #if (random.random() > eg) :\n",
    "        #    Qs = agent([state])[0];\n",
    "        #    action = np.argmax(Qs);\n",
    "            #Q = Qs[action]\n",
    "            #print(\"State: \", state, \" Qs: \", Qs.numpy(), \" Action: \", action, \" Q: \", Q.numpy())\n",
    "        #else :\n",
    "            #Qs = agent([state])[0];\n",
    "        #    action = random.randrange(2)\n",
    "            #Q = Qs[action]\n",
    "            #print(\"State: \", state, \" Action: \", action, \" Q: \", Q.numpy())\n",
    "        #print(state)\n",
    "        action, logs_pi = agent.explore([state])\n",
    "        #print(action)\n",
    "        #action = action[0].numpy(); \n",
    "        #logs_pi = logs_pi[0] \n",
    "        #print(action)\n",
    "        #print(logs_pi)\n",
    "\n",
    "        state_next, reward, done, info = env.step(action.numpy())\n",
    "        if done : reward = -1.0\n",
    "        \n",
    "        epoch_memory.append({ \"s\": state, \"a\": action, \"r\": reward, \"l\": logs_pi, \"d\": done, \"sn\": state_next })\n",
    "        \n",
    "        #mem = { \"s\": state, \"a\": action, \"r\": reward, \"d\": done, \"sn\": state_next, }        \n",
    "        #memory.append(mem)\n",
    "        \n",
    "        state = state_next\n",
    "\n",
    "        if done :\n",
    "            reward = 0\n",
    "            for mem in numpy.flip(epoch_memory) :\n",
    "                reward = reward * GAMMA + mem['r']\n",
    "                mem['r'] = reward\n",
    "            \n",
    "            memory.extend(epoch_memory)\n",
    "            #print('-' if abs(state[0])>2.4 else 'a' if abs(state[2])>0.2094 else '?')\n",
    "            print(' ', reward)\n",
    "            break\n",
    "        else :\n",
    "            print('.', end='')\n",
    "        \n",
    "        state = state_next\n",
    "        \n",
    "    if True or len(memory) >= MEMORY_SIZE :\n",
    "        \n",
    "        tf_adv = [ mem['r'] for mem in memory ]\n",
    "        \n",
    "        #logp_a_op = tf.reduce_sum(tf.one_hot(tf_a, depth=action_space) * logp_all, axis=1)\n",
    "        logp_a_op = [ mem['l'][mem['a']] for mem in memory ]\n",
    "        \n",
    "        #self.pi_loss = PolicyGradient.net_objectives(\n",
    "        #    tf_adv=self.tf_adv,\n",
    "        #    logp_a_op=self.logp_a_op\n",
    "        #)\n",
    "        pi_loss = -tf.reduce_mean(logp_a_op * tf_adv)\n",
    "        \n",
    "        train_pi = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(pi_loss)\n",
    "        \n",
    "        approx_ent = tf.reduce_mean(-logp_a_op)\n",
    "        \n",
    "        print(train_pi.numpy(), approx_ent.numpy())\n",
    "        \n",
    "        \n",
    "    break\n",
    "    #drawAgent(agent)\n",
    "    #scores.append(step)\n",
    "    #print(\"step : \", step, \" (\", np.min(scores), \" / \", np.average(scores[max(0,len(scores)-100):]), \" / \", np.max(scores), \")\")\n",
    "    #print(memory)\n",
    "    #eg = max(EXPLORATION_MIN, eg * EXPLORATION_DECAY)\n",
    "    #print(\"\\r\\neG: \", eg, \"; \", c0,\"-\",c1,\"/\",c0+c1)\n",
    "    #print(\"\\r\\neG: \", eg)\n",
    "    \n",
    "    #plt.figure(figsize=(15.0,7.0))\n",
    "    #fig, axes = plt.subplots(nrows=1, ncols=2) \n",
    "    #fig.set_size_inches(15.0, 7.0)         \n",
    "    #axes[0].plot(loss, label=\"loss\")\n",
    "    #axes[0].legend()\n",
    "    #axes[1].plot(accuracy, label=\"accuracy\")\n",
    "    #axes[1].legend()\n",
    "    #plt.show()\n",
    "    #loss = []\n",
    "    #accuracy = []\n",
    "\n",
    "    #showHistory(history)\n",
    "    \n",
    "    #print(agent([s0])[0].numpy(), \" <= \", o0)\n",
    "    #break\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eg = 0.05\n",
    "env.close()\n",
    "#plt.plot(scores)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0]\n",
      " [2]], shape=(2, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "toto = tf.random.categorical([[0.2 ,0.2, 0.6],[0.2 ,0.2, 0.6]], 1)\n",
    "print (toto)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = 0\n",
    "b = '<' if a<0 else '>' if a>0 else '0'\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "state = env.reset() #[2:]\n",
    "i=0\n",
    "while True :\n",
    "    env.render()\n",
    "\n",
    "    Qs = agent([state])[0];\n",
    "    action = np.argmax(Qs);\n",
    "    Q = Qs[action]\n",
    "    print(state, \" \", Qs.numpy(), \" \", action)\n",
    "    #drawAgent(agent, state[0], state[1])\n",
    "    \n",
    "    state, rewards, done, info = env.step(action)\n",
    "    #input()\n",
    "    i += 1\n",
    "    if (done and i>2000) or abs(state[2])>0.4 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "memory = [] #{ \"states\":[], \"masks\":[], \"actions\":[], \"values\":[]}\n",
    "loss = []\n",
    "accuracy = []\n",
    "\n",
    "state = env.reset()\n",
    "for step in range(1000):\n",
    "    env.render()\n",
    "    #print(state)\n",
    "    if (random.random() > eg) :\n",
    "        c0 +=1\n",
    "        Qs = agent([state])[0];\n",
    "        action = np.argmax(Qs);\n",
    "        Q = Qs[action]\n",
    "        #print(\"State: \", state, \" Qs: \", Qs.numpy(), \" Action: \", action, \" Q: \", Q.numpy())\n",
    "    else :\n",
    "        c1 +=1\n",
    "        Qs = agent([state])[0];\n",
    "        action = random.randrange(2)\n",
    "        Q = Qs[action]\n",
    "        #print(\"State: \", state, \" Action: \", action, \" Q: \", Q.numpy())\n",
    "\n",
    "    mask = np.zeros(2)\n",
    "    mask[action] = 1\n",
    "    mem = { \"s\": state, \"a\": action, \"m\": mask}\n",
    "    print(state, \" \", Qs.numpy(), \" \", action)\n",
    "    drawAgent(agent)\n",
    "    state, rewards, done, info = env.step(action)\n",
    "\n",
    "    if done : rewards = 0.0\n",
    "    mem[\"r\"] = rewards\n",
    "    mem[\"sp\"] = state\n",
    "    mem[\"d\"] = done\n",
    "    memory.append(mem)\n",
    "    \n",
    "    if done : \n",
    "        print(memory)\n",
    "        break\n",
    "\n",
    "\n",
    "batch = memory #random.sample(memory, BATCH_SIZE)\n",
    "bs = [ b[\"s\"] for b in batch ]\n",
    "bm = [ b[\"m\"] for b in batch ]\n",
    "by = [ b[\"m\"] * ( b['r'] if b['d'] else b['r'] + a * np.max(agent([b['sp']])[0]) )\n",
    "       for b in batch ]\n",
    "\n",
    "print (bs, bm, by)\n",
    "#history = coach.fit(\n",
    "#    [np.array(bs), np.array(bm)],\n",
    "#    np.array(by),\n",
    "#    verbose=0)\n",
    "#loss.append(history.history['loss'])\n",
    "#accuracy.append(history.history['accuracy'])\n",
    "\n",
    "drawAgent(agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
